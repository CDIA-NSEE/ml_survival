{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNoEsD6afOg94Mj2oOOnlsO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import matplotlib\n","\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n","from sklearn.metrics import roc_curve, roc_auc_score, auc, ConfusionMatrixDisplay, classification_report, confusion_matrix\n","from sklearn.inspection import permutation_importance\n","from sksurv.metrics import concordance_index_censored, concordance_index_ipcw, brier_score\n","\n","import xgboost as xgb\n","\n","from lifelines.statistics import logrank_test\n","from lifelines import KaplanMeierFitter"],"metadata":{"id":"7JijOJftgnBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmRetyJtf_6x"},"outputs":[],"source":["def train_preprocessing(df, ohe_encoder=None, normalizer='StandardScaler'):\n","    \"\"\"\n","    Preprocesses the dataset for machine learning model training.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame containing the data to be preprocessed.\n","    ohe_encoder : list, optional\n","        List of categorical columns to be transformed with One-Hot Encoding.\n","        If `None`, this step will be skipped. Default is `None`.\n","    normalizer : str, optional\n","        Normalization method to be applied. Choose between:\n","        - 'StandardScaler': Standard normalization (mean=0, std=1).\n","        - 'MinMaxScaler': Scales data to range [0, 1].\n","        - 'MaxAbsScaler': Scales data by maximum absolute value.\n","        - 'QuantileTransformer': Transforms data to normal distribution.\n","        Default is 'StandardScaler'.\n","\n","    Returns:\n","    --------\n","    df_aux : numpy.ndarray\n","        Numpy matrix containing the preprocessed and normalized data.\n","    enc : dict\n","        Dictionary containing the encoders used for categorical columns.\n","        May include `OneHotEncoder` and `OrdinalEncoder`.\n","    norm : sklearn.preprocessing.TransformerMixin\n","        Normalizer object used (such as `StandardScaler`, `MinMaxScaler`, etc.).\n","    feat_cols : pandas.Index\n","        List of columns/features present after preprocessing.\n","\n","    Notes:\n","    ------\n","    - Columns specified in `ohe_encoder` are transformed using `OneHotEncoder`.\n","    - Remaining categorical columns are encoded with `OrdinalEncoder`.\n","    - Normalization is applied to all columns after categorical encoding.\n","    \"\"\"\n","    # Data Transformation: One-Hot Encoding, Ordinal Encoding, and Normalization\n","    df_aux = df.copy()  # Create a copy of the input DataFrame\n","\n","    enc = dict()  # Initialize a dictionary to store encoders\n","\n","    # Apply One-Hot Encoding (if specified)\n","    if ohe_encoder is not None:\n","        for col in ohe_encoder:  # Iterate through columns to be one-hot encoded\n","            enc[col] = OneHotEncoder(handle_unknown='ignore', drop='first')  # Initialize and fit OneHotEncoder\n","            ohe_results = enc[col].fit_transform(df_aux[[col]])  # Apply one-hot encoding and transform\n","            df1 = pd.DataFrame(ohe_results.toarray(),  # Create DataFrame from encoded results\n","                            columns=enc[col].get_feature_names_out(),\n","                            index=df_aux[col].index)\n","            df_aux = df_aux.merge(df1, how='left', left_index=True, right_index=True)  # Merge encoded columns into main DataFrame\n","\n","        df_aux.drop(columns=ohe_encoder, inplace=True)  # Drop original one-hot encoded columns\n","\n","\n","    # Apply Ordinal Encoding to remaining categorical columns\n","    list_categorical = df_aux.select_dtypes(include='object').columns  # Identify categorical columns\n","    for col in list_categorical:  # Iterate through categorical columns\n","        enc[col] = OrdinalEncoder(handle_unknown='use_encoded_value',  # Initialize and fit OrdinalEncoder\n","                                unknown_value=-1)  # Handle unknown values by assigning -1\n","        df_aux[col] = enc[col].fit_transform(df_aux[[col]])  # Apply ordinal encoding and transform\n","\n","    feat_cols = df_aux.columns  # Store the feature names after encoding\n","\n","    # Apply Normalization based on the specified method\n","    if normalizer == 'StandardScaler':\n","        norm = StandardScaler()\n","    elif normalizer == 'MinMaxScaler':\n","        norm = MinMaxScaler((0, 1))\n","    elif normalizer == 'MaxAbsScaler':\n","        norm = MaxAbsScaler()\n","    elif normalizer == 'QuantileTransformer':\n","        norm = QuantileTransformer(output_distribution='normal')\n","\n","    df_aux = norm.fit_transform(df_aux)  # Fit and transform the data using the selected scaler\n","\n","    return df_aux, enc, norm, feat_cols  # Return the transformed DataFrame, encoders, normalizer, and feature names\n","\n","################################################################################\n","\n","def test_preprocessing(df, enc, norm, ohe_encoder=None):\n","    \"\"\"\n","    Preprocesses the test dataset using the transformers fitted on the training set.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame containing the test data to be preprocessed.\n","    enc : dict\n","        Dictionary containing the encoders fitted on the training set.\n","        May include `OneHotEncoder` and `OrdinalEncoder` for categorical columns.\n","    norm : sklearn.preprocessing.TransformerMixin\n","        Normalizer object fitted on the training set (such as `StandardScaler`, `MinMaxScaler`, etc.).\n","    ohe_encoder : list, optional\n","        List of categorical columns that were transformed with One-Hot Encoding during training.\n","        If `None`, this step will be skipped. Default is `None`.\n","\n","    Returns:\n","    --------\n","    df_aux : numpy.ndarray\n","        Numpy matrix containing the preprocessed and normalized test data.\n","\n","    Notes:\n","    ------\n","    - Columns specified in `ohe_encoder` are transformed using the `OneHotEncoder` fitted on the training set.\n","    - Remaining categorical columns are transformed using the `OrdinalEncoder` fitted on training.\n","    - Normalization is applied to all columns after categorical encoding.\n","    - It's important to ensure that `enc` and `norm` are the same ones used in preprocessing the training set.\n","    \"\"\"\n","    # Data Transformation: One-Hot Encoding and Normalization\n","    df_aux = df.copy()  # Create a copy of the input DataFrame\n","\n","    # Apply One-Hot Encoding (if encoder is provided)\n","    if ohe_encoder is not None:\n","        for col in ohe_encoder: # Iterate through categorical columns to be encoded\n","            ohe_results = enc[col].transform(df_aux[[col]])  # Apply one-hot encoding\n","            df1 = pd.DataFrame(ohe_results.toarray(),  # Create a new DataFrame with the encoded features\n","                            columns=enc[col].get_feature_names_out(),\n","                            index=df_aux[col].index)\n","            df_aux = df_aux.merge(df1, how='left', left_index=True, right_index=True)  # Merge encoded features into the main DataFrame\n","\n","        df_aux.drop(columns=ohe_encoder, inplace=True)  # Remove original categorical columns\n","\n","    # Apply Encoding to other categorical features\n","    list_categorical = df_aux.select_dtypes(include='object').columns #Get the list of categorical columns\n","    for col in list_categorical: #Iterate through the categorical columns\n","        df_aux[col] = enc[col].transform(df_aux[[col]]) #Apply the transformation\n","\n","    # Apply Normalization\n","    df_aux = norm.transform(df_aux)  # Apply normalization to the entire DataFrame\n","\n","    return df_aux  # Return the transformed DataFrame\n","\n","################################################################################\n","\n","def preprocessing(x_train, x_test, ohe_encoder=None, norm_name='StandardScaler',\n","                  return_enc_norm=False, random_state=0):\n","    \"\"\"\n","    Preprocesses both training and test datasets, including category encoding and normalization.\n","\n","    Parameters:\n","    -----------\n","    x_train : pandas.DataFrame\n","        Training dataset.\n","    x_test : pandas.DataFrame\n","        Test dataset.\n","    ohe_encoder : list, optional\n","        List of categorical columns to be transformed with One-Hot Encoding.\n","        If `None`, this step will be skipped. Default is `None`.\n","    norm_name : str, optional\n","        Name of the normalization method to be applied. Choose between:\n","        - 'StandardScaler': Standard normalization (mean=0, std=1).\n","        - 'MinMaxScaler': Scales data to range [0, 1].\n","        - 'MaxAbsScaler': Scales data by maximum absolute value.\n","        - 'QuantileTransformer': Transforms data to normal distribution.\n","        Default is 'StandardScaler'.\n","    return_enc_norm : bool, optional\n","        If `True`, also returns the fitted `enc` (encoders) and `norm` (normalizer) objects.\n","        Default is `False`.\n","    random_state : int, optional\n","        Seed to ensure reproducibility in preprocessing.\n","        Default is `0`.\n","\n","    Returns:\n","    --------\n","    X_train_ : numpy.ndarray\n","        Numpy matrix containing the preprocessed and normalized training data.\n","    X_test_ : numpy.ndarray\n","        Numpy matrix containing the preprocessed and normalized test data.\n","    feat_cols : pandas.Index\n","        List of columns/features present after preprocessing.\n","    enc : dict, optional\n","        Dictionary containing the encoders fitted on the training set (returned if `return_enc_norm=True`).\n","    norm : sklearn.preprocessing.TransformerMixin, optional\n","        Normalizer object fitted on the training set (returned if `return_enc_norm=True`).\n","\n","    Notes:\n","    ------\n","    - The function internally uses `train_preprocessing` to fit the encoders and normalizers on the training set.\n","    - The test set is transformed using the same encoders and normalizers fitted on training.\n","    - If `return_enc_norm=False`, only the preprocessed data and feature columns are returned.\n","    \"\"\"\n","    # Data Preprocessing\n","    # Apply preprocessing steps to the training and testing data\n","    X_train_, enc, norm, feat_cols = train_preprocessing(x_train,\n","                                                         ohe_encoder=ohe_encoder,\n","                                                         normalizer=norm_name)\n","    X_test_ = test_preprocessing(x_test, enc, norm, ohe_encoder=ohe_encoder)\n","\n","    # Return preprocessed data and preprocessing objects (encoder and normalizer) if requested\n","    if return_enc_norm:\n","        return X_train_, X_test_, feat_cols, enc, norm\n","    else:\n","        return X_train_, X_test_, feat_cols\n","\n","################################################################################\n","\n","def plot_survival_curve(x, y, name, compare_km=False, test_df=None):\n","    \"\"\"\n","    Plots the survival curve for the provided data.\n","\n","    Parameters:\n","    -----------\n","    x : array-like\n","        Vector representing the analysis times.\n","    y : array-like\n","        Vector representing the survival probabilities associated with each time in `x`.\n","    name : str\n","        Name or label for the survival curve.\n","    compare_km : bool, optional\n","        If True, adds the Kaplan-Meier (KM) curve to the plot for comparison with the provided curve.\n","\n","    Returns:\n","    --------\n","    None\n","        The function doesn't return any value, but displays a plot of the survival curve.\n","\n","    Notes:\n","    ------\n","    - If `compare_km=True`, the Kaplan-Meier curve will be calculated using the variables\n","      `test_df.obito_geral` (event indicator) and `test_df.meses_diag` (follow-up time).\n","    - The plot displays the survival function on the y-axis and time (in months) on the x-axis.\n","    \"\"\"\n","    # Survival Curve Plotting\n","    fig = plt.figure(figsize=(12, 6))  # Create the plot figure\n","    ax = plt.subplot(111)  # Create the axes for the plot\n","\n","    # Kaplan-Meier Comparison (if enabled)\n","    if compare_km:\n","        E = (test_df.obito_geral == 1)  # Extract event data (death)\n","        T = test_df.meses_diag  # Extract time-to-event data (months since diagnosis)\n","\n","        kmf = KaplanMeierFitter()  # Initialize Kaplan-Meier model\n","        ax = kmf.fit(T, E).plot_survival_function(ax=ax, ci_show=False,  # Fit and plot Kaplan-Meier survival curve\n","                                                show_censors=True,  # Show censoring events\n","                                                label='Kaplan-Meier')  # Set label for the curve\n","        title = f'Kaplan Meier vs {name}'  # Set title for comparison plot\n","\n","    else:\n","        title = name  # Set title if no comparison\n","\n","    # Plotting Model Survival Curve\n","    ax.step(x, y, where='post', label=name)  # Plot the model's survival curve\n","\n","    # Plot Formatting\n","    ax.set_title(title)  # Set the plot title\n","    ax.set_xlabel('Months after diagnosis')  # Set x-axis label\n","    ax.set_ylabel('Survival function')  # Set y-axis label\n","    ax.spines['top'].set_visible(False)  # Remove top border\n","    ax.spines['right'].set_visible(False)  # Remove right border\n","    plt.xticks(np.arange(0, x[-1] + 11, 12))  # Set x-axis ticks every 12 months\n","    plt.ylim([0, 1.1])  # Set y-axis limits\n","    plt.legend();  # Show the legend\n","\n","################################################################################\n","\n","def c_index_ipcw_brier_score(model, X_train, y_train, X_test, y_test):\n","    \"\"\"\n","    Calculates the IPCW Concordance Index, time-dependent Brier Score, and Integrated Brier Score (IBS) for a survival model.\n","\n","    Parameters:\n","    -----------\n","    model : object\n","        Trained survival model that supports `predict` (risk) and\n","        `predict_survival_function` (survival curves) methods.\n","    X_train : array-like\n","        Input features of the training set.\n","    y_train : pandas.DataFrame\n","        Training set containing `event` (event indicator) and `time` (time to event or censoring) columns.\n","    X_test : array-like\n","        Input features of the test set.\n","    y_test : pandas.DataFrame\n","        Test set containing `event` (event indicator) and `time` (time to event or censoring) columns.\n","\n","    Returns:\n","    --------\n","    None\n","        The function doesn't return values directly, but displays the following results in the console and plots:\n","        - IPCW C-Index for training and test sets.\n","        - Time-dependent Brier Score.\n","        - Integrated Brier Score (IBS).\n","        - Brier Scores at 1, 3, and 5 years.\n","        - Plot of the Brier Score over time.\n","\n","    Notes:\n","    ------\n","    - The IPCW (Inverse Probability of Censoring Weighting) C-Index is an adjusted version of the concordance index to handle censored data.\n","    - The Brier Score measures prediction accuracy over time. The IBS is the integral of the Brier Score over the entire follow-up period.\n","    - Model predictions are based on risk and survival curves for the test set.\n","    \"\"\"\n","    # Model Evaluation: Predictions and Concordance Index\n","    # Generate predictions (risk or survival curves) for train and test sets\n","    surv_risks = model.predict(X_test)\n","    surv_risks_train = model.predict(X_train)\n","\n","    # Calculate the IPCW Concordance Index for both test and train sets\n","    c_index_ipcw = concordance_index_ipcw(y_train, y_test, surv_risks)\n","    c_index_ipcw_train = concordance_index_ipcw(y_train, y_train, surv_risks_train)\n","    print(f'C-Index IPCW Test: {c_index_ipcw[0]}')\n","    print(f'C-Index IPCW Train: {c_index_ipcw_train[0]}')\n","\n","\n","    # Model Evaluation: Brier Score and IBS\n","    # Predict survival curves for the test set\n","    surv_curves = model.predict_survival_function(X_test, return_array=True)\n","\n","    # Calculate the Brier Score at various time points\n","    time_points = model.unique_times_[:-1]  # Define the time points for evaluation\n","    brier_scores = brier_score(y_train, y_test, surv_curves[:, :-1], time_points)\n","\n","    # Calculate the Integrated Brier Score (IBS)\n","    time_differences = np.diff(np.hstack([[0], time_points]))\n","    ibs = np.sum(brier_scores[1] * time_differences) / time_points[-1]\n","    print(f'\\nIntegrated Brier Score (IBS): {ibs}')\n","\n","    # Print Brier Score at specific time points (1, 3, and 5 years)\n","    print(f'\\nBrier Score:')\n","    print(f'    > 1 year = {brier_scores[1][11]}')\n","    print(f'    > 3 years = {brier_scores[1][35]}')\n","    print(f'    > 5 years = {brier_scores[1][59]}\\n')\n","\n","    # Plotting the Brier Score\n","    plt.figure(figsize=(10, 6))  # Create the plot figure\n","    plt.plot(brier_scores[0], brier_scores[1], label='Brier Score')  # Plot the Brier Score over time\n","    plt.xlabel('Time (months)')  # Set x-axis label\n","    plt.ylabel('Brier Score')  # Set y-axis label\n","    # plt.title('Brier Score over time')  # Optional plot title\n","    plt.legend()  # Show the plot legend\n","    plt.show()  # Display the plot\n","\n","################################################################################\n","\n","def media_mediana(time_bins, surv_curves, is_xgbse=False):\n","    \"\"\"\n","    Calculates and displays survival statistics (mean and median) based on the provided survival curves.\n","\n","    Parameters:\n","    -----------\n","    time_bins : array-like\n","        Vector containing the times corresponding to the analysis intervals of survival curves.\n","    surv_curves : array-like\n","        Matrices or lists containing survival curves for each individual or group.\n","    is_xgbse : bool, optional\n","        Indicates whether the survival curves were generated by the XGBSE model. If True, curves are converted to NumPy arrays.\n","\n","    Returns:\n","    --------\n","    None\n","        The function doesn't return any value, but prints:\n","        - The number of patients with survival probability below 50% at some point.\n","        - The mean and median survival times for a 50% probability.\n","        - The times for mean and median survival curves.\n","\n","    Notes:\n","    ------\n","    - The median is the time when survival probability reaches or falls below 50%.\n","    - Mean and median curves are calculated by aggregating the provided curves.\n","    \"\"\"\n","    # Calculate survival times for individual curves\n","    surv_times = []\n","\n","    if is_xgbse:\n","        surv_curves = surv_curves.values\n","\n","    for curve in surv_curves:\n","        id = np.where(curve <= 0.5)[0]  # Find indices where survival probability is <= 50%\n","\n","        if len(id) > 0:\n","            surv_times.append(time_bins[id[0]])  # Append the corresponding time\n","\n","    print(f'Number of patients with less than 50% survival at some point: {len(surv_times)}')\n","\n","    print(f'    Mean: {np.mean(surv_times):.2f} Months')  # Print mean survival time\n","    print(f'    Median: {np.median(surv_times):.2f} Months')  # Print median survival time\n","\n","\n","    # Calculate and print mean and median survival times from aggregated curves\n","    surv_mean = surv_curves.mean(axis=0)  # Calculate mean survival curve\n","    id_media = np.where(surv_mean <= 0.5)[0]  # Find index where mean survival probability is <= 50%\n","    if len(id_media) > 0:\n","        media = time_bins[id_media[0]]  # Get time from mean curve\n","    else:\n","        media = 0\n","\n","    surv_median = np.median(surv_curves, axis=0)  # Calculate median survival curve\n","    id_median = np.where(surv_median <= 0.5)[0]  # Find index where median survival probability is <= 50%\n","    if len(id_median) > 0:\n","        mediana = time_bins[id_median[0]]  # Get time from median curve\n","    else:\n","        mediana = 0\n","\n","    print(f'\\nTimes of the mean and median curves:')\n","    print(f'    Mean: {media:.2f} Months')  # Print mean survival time\n","    print(f'    Median: {mediana:.2f} Months')  # Print median survival time\n","\n","################################################################################\n","\n","def c_index_scorer(estimator, X, y):\n","    \"\"\"\n","    Calculates the Concordance Index (C-Index) for a survival model.\n","\n","    Parameters:\n","    -----------\n","    estimator : object\n","        Trained estimator model. It must support the `predict_survival_function` or `predict` methods.\n","    X : array-like\n","        Input data for model predictions.\n","    y : pandas.DataFrame\n","        Output data containing `event` (event indicator) and `time` (time to event or censoring) columns.\n","\n","    Returns:\n","    --------\n","    c_index : float\n","        Concordance Index (C-Index) value, ranging from 0 to 1.\n","        Values closer to 1 indicate better discrimination ability of the model.\n","\n","    Notes:\n","    ------\n","    - The function attempts to use the estimator's `predict_survival_function` method to obtain survival curve predictions.\n","      If this method is unavailable, it uses the `predict` method to obtain risk scores.\n","    - The C-Index evaluates the model's ability to correctly rank relative survival times between samples.\n","    - Ensure that the `y` object is correctly formatted, with the necessary columns.\n","    \"\"\"\n","    try:\n","        # Try to use the `predict_survival_function` method (preferred if available and returns numpy array)\n","        y_pred = estimator.predict_survival_function(X, return_array=True)\n","\n","        # Check if y_pred is a numpy array; if not, try predict\n","        if not isinstance(y_pred, np.ndarray):\n","            raise AttributeError(\"predict_survival_function did not return a NumPy array; falling back to predict\")\n","\n","    except AttributeError:\n","        # If the `predict_survival_function` method does not exist or does not return numpy array, use the `predict` method (risk scores)\n","        y_pred = estimator.predict(X)\n","\n","    # Calculate and return the C-Index\n","    # The concordance_index function expects risk scores where higher values = shorter survival\n","    c_index = concordance_index(y['event'], y['time'], y_pred)  # Pass event and time separately\n","\n","    return c_index\n","\n","################################################################################\n","\n","def c_index_metric(y_true, y_pred):\n","    \"\"\"\n","    Calculates the Concordance Index (C-Index) to evaluate survival models.\n","\n","    Parameters:\n","    -----------\n","    y_true : pandas.DataFrame\n","        DataFrame containing the ground truth survival data.\n","        It must include the following columns:\n","        - `event` : bool\n","            Event indicator (True for event occurred, False for censoring).\n","        - `time` : float\n","            Time to event or censoring.\n","    y_pred : array-like\n","        Model predicted values, representing risks or another related metric.\n","\n","    Returns:\n","    --------\n","    c_index : float\n","        Concordance Index (C-Index) value, ranging from 0 to 1.\n","        Values closer to 1 indicate better discrimination ability of the model.\n","\n","    Notes:\n","    ------\n","    - The Concordance Index measures the model's ability to correctly rank relative survival times\n","      between samples.\n","    - For a reliable C-Index, `y_pred` should reflect the ordering of risks or expected survival times.\n","    \"\"\"\n","    # Extract event and time information from y_true\n","    event = y_true['event']\n","    time = y_true['time']\n","\n","    # Calculate the C-Index.\n","    c_index = concordance_index_censored(event, time, y_pred)[0]  # [0] gets the c-index, other values are related to ties.\n","\n","    return c_index\n","\n","################################################################################\n","\n","def c_index_metric_lgbm(y_true, y_pred):\n","    \"\"\"\n","    Calculates the Concordance Index (C-Index) to evaluate survival models\n","    trained with LightGBM.\n","\n","    Parameters:\n","    -----------\n","    y_true : pandas.DataFrame\n","        DataFrame containing the ground truth survival data.\n","        It must include the following columns:\n","        - `event` : bool\n","            Event indicator (True for event occurred, False for censoring).\n","        - `time` : float\n","            Time to event or censoring.\n","    y_pred : array-like\n","        Values predicted by the LightGBM model, representing risks\n","        (higher values indicate greater risk).\n","\n","    Returns:\n","    --------\n","    c_index : float\n","        Concordance Index (C-Index) value, ranging from 0 to 1.\n","        Values closer to 1 indicate better discrimination ability of the model.\n","\n","    Notes:\n","    ------\n","    - LightGBM returns risks directly, but to calculate the C-Index, we use the negative of the risks (`-y_pred`),\n","      since the C-Index assumes that higher values correspond to longer survival times.\n","    - Ensure that `y_pred` is aligned with the data in `y_true`.\n","    \"\"\"\n","    # Extract event and time information from y_true\n","    event = y_true['event']\n","    time = y_true['time']\n","\n","    # Calculate the C-Index. Note the negation of y_pred as higher values = greater risk\n","    c_index = concordance_index_censored(event, time, -y_pred)[0]  # [0] gets the c-index, other values are related to ties.\n","\n","    return c_index\n","\n","################################################################################\n","\n","def calculate_permutation_importance(model, X_test, y_test, feat_cols,\n","                                     n_repeats=10, random_state=42):\n","    \"\"\"\n","    Calculates feature importance of a model using the permutation technique.\n","\n","    Permutation importance is calculated as the reduction in model performance (using C-Index)\n","    when randomly shuffling the values of a feature in the test set.\n","\n","    Parameters:\n","    -----------\n","    model : object\n","        Trained model that implements the `predict` method. The model should predict risks\n","        or related values for survival evaluation.\n","    X_test : pandas.DataFrame or numpy.ndarray\n","        Test dataset containing the features.\n","        If it's a NumPy array, it will be converted to a DataFrame using `feat_cols` as column names.\n","    y_test : pandas.DataFrame\n","        DataFrame containing the ground truth survival data:\n","        - Column `event`: indicates whether the event occurred (1) or was censored (0).\n","        - Column `time`: time to event or censoring.\n","    feat_cols : list\n","        List with the names of the features present in `X_test`.\n","    n_repeats : int, optional\n","        Number of repetitions for the permutation of each feature. Defaults to 10.\n","    random_state : int, optional\n","        Seed for random number generation, ensuring reproducibility.\n","\n","    Returns:\n","    --------\n","    importance_df : pandas.DataFrame\n","        DataFrame containing the means and standard deviations of the calculated importances\n","        for each feature.\n","        The columns include:\n","        - `importances_mean`: mean of the permutation importances.\n","        - `importances_std`: standard deviation of the permutation importances.\n","\n","    Notes:\n","    ------\n","    - The baseline is calculated using the C-Index on the test set without permutation.\n","    - Shuffling a feature reduces the information contained in the data, resulting in an expected\n","      decrease in performance.\n","    - The method is suitable for survival models that use the C-Index as a metric.\n","    \"\"\"\n","    # Ensure X_test is a DataFrame\n","    if isinstance(X_test, np.ndarray):\n","        X_test = pd.DataFrame(X_test, columns=feat_cols)\n","\n","    # Predict initial risks on the test set.  Using .values to get the numpy array\n","    y_pred = model.predict(X_test.values)  # Use .values for numpy array input\n","\n","    # Calculate the baseline C-Index (no permutation)\n","    baseline_c_index = c_index_metric(y_test, y_pred)\n","\n","    # Initialize lists to store the results\n","    importances_mean = []\n","    importances_std = []\n","\n","    # Iterate over each feature to calculate permutation importance\n","    rng = np.random.RandomState(random_state)\n","    for col in feat_cols:\n","        scores = []\n","\n","        for _ in range(n_repeats):  # Repeat permutation\n","            X_test_permuted = X_test.copy()\n","\n","            # Permute the current column\n","            X_test_permuted[col] = rng.permutation(X_test[col].values)\n","\n","            # Predict again with the permuted column. Use .values for numpy array input\n","            y_pred_permuted = model.predict(X_test_permuted.values)  # Use .values for numpy array input\n","\n","            # Calculate the new C-Index\n","            permuted_c_index = c_index_metric(y_test, y_pred_permuted)\n","\n","            # Record the difference in C-Index\n","            scores.append(baseline_c_index - permuted_c_index)\n","\n","        # Store the mean and standard deviation of the importances\n","        importances_mean.append(np.mean(scores))\n","        importances_std.append(np.std(scores))\n","\n","    # Create a DataFrame with the results\n","    importance_df = pd.DataFrame({\n","        'importances_mean': importances_mean,\n","        'importances_std': importances_std\n","    }, index=feat_cols).sort_values(by='importances_mean', ascending=False)\n","\n","    return importance_df\n","\n","################################################################################\n","\n","def calculate_permutation_importance_xgb(model, X_test, y_test, feat_cols,\n","                                         n_repeats=10, random_state=42):\n","    \"\"\"\n","    Calculates feature importance of an XGBoost model using the permutation technique.\n","\n","    Permutation importance is calculated as the reduction in model performance (using C-Index)\n","    when randomly shuffling the values of a feature in the test set.\n","\n","    Parameters:\n","    -----------\n","    model : object\n","        Trained XGBoost model that implements the `predict` method. The model should predict risks\n","        or related values for survival evaluation.\n","    X_test : pandas.DataFrame or numpy.ndarray\n","        Test dataset containing the features.\n","        If it's a NumPy array, it will be converted to a DataFrame using `feat_cols` as column names.\n","    y_test : pandas.DataFrame\n","        DataFrame containing the ground truth survival data:\n","        - Column `event`: indicates whether the event occurred (1) or was censored (0).\n","        - Column `time`: time to event or censoring.\n","    feat_cols : list\n","        List with the names of the features present in `X_test`.\n","    n_repeats : int, optional\n","        Number of repetitions for the permutation of each feature. Defaults to 10.\n","    random_state : int, optional\n","        Seed for random number generation, ensuring reproducibility.\n","\n","    Returns:\n","    --------\n","    importance_df : pandas.DataFrame\n","        DataFrame containing the means and standard deviations of the calculated importances\n","        for each feature.\n","        The columns include:\n","        - `importances_mean`: mean of the permutation importances.\n","        - `importances_std`: standard deviation of the permutation importances.\n","\n","    Notes:\n","    ------\n","    - The baseline is calculated using the C-Index on the test set without permutation.\n","    - Shuffling a feature reduces the information contained in the data, resulting in an expected\n","      decrease in performance.\n","    - The method is suitable for survival models that use the C-Index as a metric.\n","    - The model should be trained to predict survival times or risks, rather than a simple classification.\n","    \"\"\"\n","    # Ensure X_test is a DataFrame\n","    if isinstance(X_test, np.ndarray):\n","        X_test = pd.DataFrame(X_test, columns=feat_cols)\n","\n","    # Predict initial risks on the test set using a DMatrix for XGBoost\n","    dtest = xgb.DMatrix(X_test)  # Create DMatrix for efficiency with XGBoost\n","    y_pred = model.predict(dtest)\n","\n","    # Calculate the baseline C-Index (no permutation)\n","    baseline_c_index = c_index_metric(y_test, y_pred)\n","\n","    # Initialize lists to store the results\n","    importances_mean = []\n","    importances_std = []\n","\n","    # Iterate over each feature to calculate permutation importance\n","    rng = np.random.RandomState(random_state)\n","    for col in feat_cols:\n","        scores = []\n","\n","        for _ in range(n_repeats):  # Repeat permutation\n","            X_test_permuted = X_test.copy()\n","\n","            # Permute the current column\n","            X_test_permuted[col] = rng.permutation(X_test[col].values)\n","\n","            # Predict again with the permuted column using a DMatrix\n","            dtest_permuted = xgb.DMatrix(X_test_permuted) # Create DMatrix for permuted data\n","            y_pred_permuted = model.predict(dtest_permuted)\n","\n","            # Calculate the new C-Index\n","            permuted_c_index = c_index_metric(y_test, y_pred_permuted)\n","\n","            # Record the difference in C-Index\n","            scores.append(baseline_c_index - permuted_c_index)\n","\n","        # Store the mean and standard deviation of the importances\n","        importances_mean.append(np.mean(scores))\n","        importances_std.append(np.std(scores))\n","\n","    # Create a DataFrame with the results\n","    importance_df = pd.DataFrame({\n","        'importances_mean': importances_mean,\n","        'importances_std': importances_std\n","    }, index=feat_cols).sort_values(by='importances_mean', ascending=False)\n","\n","    return importance_df\n","\n","################################################################################\n","\n","def calculate_permutation_importance_lgbm(model, X_test, y_test, feat_cols,\n","                                          n_repeats=10, random_state=42):\n","    \"\"\"\n","    Calculates feature importance of a LightGBM model using the permutation technique.\n","\n","    Permutation importance is calculated as the reduction in model performance (using C-Index)\n","    when randomly shuffling the values of a feature in the test set.\n","\n","    Parameters:\n","    -----------\n","    model : object\n","        Trained LightGBM model that implements the `predict` method. The model should predict risks\n","        or related values for survival evaluation.\n","    X_test : pandas.DataFrame or numpy.ndarray\n","        Test dataset containing the features.\n","        If it's a NumPy array, it will be converted to a DataFrame using `feat_cols` as column names.\n","    y_test : pandas.DataFrame\n","        DataFrame containing the ground truth survival data:\n","        - Column `event`: indicates whether the event occurred (1) or was censored (0).\n","        - Column `time`: time to event or censoring.\n","    feat_cols : list\n","        List with the names of the features present in `X_test`.\n","    n_repeats : int, optional\n","        Number of repetitions for the permutation of each feature. Defaults to 10.\n","    random_state : int, optional\n","        Seed for random number generation, ensuring reproducibility.\n","\n","    Returns:\n","    --------\n","    importance_df : pandas.DataFrame\n","        DataFrame containing the means and standard deviations of the calculated importances\n","        for each feature.\n","        The columns include:\n","        - `importances_mean`: mean of the permutation importances.\n","        - `importances_std`: standard deviation of the permutation importances.\n","\n","    Notes:\n","    ------\n","    - The baseline is calculated using the C-Index on the test set without permutation.\n","    - Shuffling a feature reduces the information contained in the data, resulting in an expected\n","      decrease in performance.\n","    - The method is suitable for survival models that use the C-Index as a metric.\n","    - The model should be trained to predict survival times or risks, rather than a simple classification.\n","    \"\"\"\n","    # Ensure X_test is a DataFrame\n","    if isinstance(X_test, np.ndarray):\n","        X_test = pd.DataFrame(X_test, columns=feat_cols)\n","\n","    # Predict initial risks on the test set\n","    y_pred = model.predict(X_test)\n","\n","    # Calculate the baseline C-Index (no permutation)\n","    baseline_c_index = c_index_metric_lgbm(y_test, y_pred)\n","\n","    # Initialize lists to store the results\n","    importances_mean = []\n","    importances_std = []\n","\n","    # Iterate over each feature to calculate permutation importance\n","    rng = np.random.RandomState(random_state)\n","    for col in feat_cols:\n","        scores = []\n","\n","        for _ in range(n_repeats):  # Repeat permutation\n","            X_test_permuted = X_test.copy()\n","\n","            # Permute the current column\n","            X_test_permuted[col] = rng.permutation(X_test[col].values)\n","\n","            # Predict again with the permuted column\n","            y_pred_permuted = model.predict(X_test_permuted)\n","\n","            # Calculate the new C-Index\n","            permuted_c_index = c_index_metric_lgbm(y_test, y_pred_permuted)\n","\n","            # Record the difference in C-Index\n","            scores.append(baseline_c_index - permuted_c_index)\n","\n","        # Store the mean and standard deviation of the importances\n","        importances_mean.append(np.mean(scores))\n","        importances_std.append(np.std(scores))\n","\n","    # Create a DataFrame with the results\n","    importance_df = pd.DataFrame({\n","        'importances_mean': importances_mean,\n","        'importances_std': importances_std\n","    }, index=feat_cols).sort_values(by='importances_mean', ascending=False)\n","\n","    return importance_df\n","\n","################################################################################\n","\n","def predict_survival_probability(surv_risks, survival_prob, time):\n","    \"\"\"\n","    Calculates the adjusted survival probability for each individual at each time point.\n","    \"\"\"\n","    # Calculate the baseline hazard from the survival probability\n","    baseline_hazard = -np.log(survival_prob)  # Vector of size (n_time,)\n","\n","    # Align dimensions: expand surv_risks to (n_individuals, 1)\n","    surv_risks = surv_risks[:, np.newaxis]  # Now (n_individuals, 1)\n","\n","    # Multiply the risks (n_individuals, 1) by the baseline hazard (1, n_time)\n","    survival_probabilities = np.exp(-surv_risks * baseline_hazard)  # Now (n_individuals, n_time)\n","\n","    return survival_probabilities"]}]}